package com.example.voiptest

import android.content.Context
import android.os.Process
import android.util.Log
import kotlinx.coroutines.CoroutineScope
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.launch
import kotlinx.coroutines.withContext
import org.pytorch.IValue
import org.pytorch.LiteModuleLoader
import org.pytorch.Module
import org.pytorch.Tensor
import java.io.File
import java.io.FileOutputStream
import java.nio.ByteBuffer
import java.nio.ByteOrder
import java.nio.ShortBuffer
import java.util.concurrent.atomic.AtomicBoolean
import android.os.SystemClock

/**
 * ë”¥í˜ì´í¬ ì˜¤ë””ì˜¤ íƒì§€ í´ë˜ìŠ¤
 * PTL ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì—ì„œ ë”¥í˜ì´í¬ë¥¼ íƒì§€í•©ë‹ˆë‹¤.
 * íŠ¬ë¸”ë§ ìœˆë„ìš° ë°©ì‹: 3ì´ˆ ë²„í¼ë¥¼ ì±„ìš°ê³  ë¶„ì„ í›„ ë¹„ì›€ (ê²¹ì¹¨ ì—†ìŒ)
 */
class DeepfakeDetector(
    private val context: Context,
    private val onResult: (isReal: Boolean, confidence: Float) -> Unit
) {
    private var model: Module? = null
    private val audioBuffer = mutableListOf<Short>()
    private val pendingBuffer = mutableListOf<Short>()  // ë¶„ì„ ì¤‘ì— ë“¤ì–´ì˜¤ëŠ” ìƒ˜í”Œ ì„ì‹œ ì €ì¥
    private val isAnalyzing = AtomicBoolean(false)

    // Statistics
    private var inferenceCount = 0L
    private var totalInferenceTimeMs = 0L
    private var totalCpuTimeMs = 0L

    companion object {
        private const val TAG = "DeepfakeDetector"
        private const val MODEL_NAME = "deepfake_audio_detection_quantized.ptl"
        private const val SAMPLE_RATE = 16000
        private const val BUFFER_SIZE_SECONDS = 2
        private const val TARGET_BUFFER_SIZE = SAMPLE_RATE * BUFFER_SIZE_SECONDS // 48000 ìƒ˜í”Œ (3ì´ˆ)

        // VAD (Voice Activity Detection) ì„ê³„ê°’
        // ì¸¡ì •ê°’: ë¬´ìŒ í‰ê·  4.36, ìŒì„± í‰ê·  108.4
        // ì„ê³„ê°’ì„ ìŒì„± í‰ê· ì˜ ì•½ 65%ë¡œ ì„¤ì •í•˜ì—¬ ë…¸ì´ì¦ˆ í•„í„°ë§ ê°•í™”
        private const val RMS_THRESHOLD = 70.0

        // â­ ì¶”ê°€: 3ì´ˆ ë²„í¼ ì „ì²´ì˜ RMSê°€ ì´ ê°’ë³´ë‹¤ ë‚®ìœ¼ë©´ ë¶„ì„ì„ SKIP
        private const val FINAL_BUFFER_RMS_THRESHOLD = 15.0
    }

    init {
        loadModel()
    }

    /**
     * Assetsì—ì„œ PTL ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
     */
    private fun loadModel() {
        try {
            Log.d(TAG, "ëª¨ë¸ ë¡œë“œ ì‹œì‘: $MODEL_NAME")

            // Assetsì—ì„œ ì„ì‹œ íŒŒì¼ë¡œ ë³µì‚¬
            val modelFile = assetFilePath(context, MODEL_NAME)

            // PyTorch Lite ëª¨ë¸ ë¡œë“œ
            model = LiteModuleLoader.load(modelFile)

            Log.d(TAG, "âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        } catch (e: Exception) {
            Log.e(TAG, "âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨", e)
        }
    }

    /**
     * Assets íŒŒì¼ì„ ì„ì‹œ íŒŒì¼ë¡œ ë³µì‚¬í•˜ì—¬ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
     */
    private fun assetFilePath(context: Context, assetName: String): String {
        val file = File(context.filesDir, assetName)

        // ì´ë¯¸ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ë°”ë¡œ ë°˜í™˜
        if (file.exists()) {
            return file.absolutePath
        }

        // Assetsì—ì„œ íŒŒì¼ ë³µì‚¬
        context.assets.open(assetName).use { inputStream ->
            FileOutputStream(file).use { outputStream ->
                val buffer = ByteArray(4 * 1024)
                var read: Int
                while (inputStream.read(buffer).also { read = it } != -1) {
                    outputStream.write(buffer, 0, read)
                }
                outputStream.flush()
            }
        }

        return file.absolutePath
    }

    /**
     * remoteSttPortì—ì„œ ë°›ì€ ByteArrayë¥¼ ë²„í¼ì— ì¶”ê°€í•©ë‹ˆë‹¤.
     * íŠ¬ë¸”ë§ ìœˆë„ìš° ë°©ì‹: ë¶„ì„ ì¤‘ì´ë©´ pendingBufferì—, ì•„ë‹ˆë©´ audioBufferì— ì €ì¥
     * @param byteArray 16-bit PCM ì˜¤ë””ì˜¤ ë°ì´í„°
     */
    fun addAudioChunk(byteArray: ByteArray) {
        // ByteArrayë¥¼ Short ë°°ì—´ë¡œ ë³€í™˜ (16-bit PCM)
        val shortArray = ByteBuffer.wrap(byteArray)
            .order(ByteOrder.LITTLE_ENDIAN)
            .asShortBuffer()

        // âœ… 1ë‹¨ê³„: RMS ì—ë„ˆì§€ ë ˆë²¨ ì¸¡ì •
        val rms = calculateRMS(shortArray)

        // âœ… 2ë‹¨ê³„: ìŒì„± í™œë™ ê°ì§€ (VAD) - ì„ì‹œ ë¹„í™œì„±í™”
        // if (!isVoiceActive(rms)) {
        //     Log.d(TAG, "â­ï¸ ë¬´ìŒ ê°ì§€ (RMS: %.2f < %.2f) â†’ ë¶„ì„ SKIP".format(rms, RMS_THRESHOLD))
        //     return
        // }

        // ByteArrayë¥¼ Short ë°°ì—´ë¡œ ë‹¤ì‹œ ë³€í™˜ (positionì´ ì´ë¯¸ ì´ë™í–ˆìœ¼ë¯€ë¡œ)
        shortArray.rewind()

        synchronized(audioBuffer) {
            if (isAnalyzing.get()) {
                // âœ… ë¶„ì„ ì¤‘ì´ë©´ ì„ì‹œ ë²„í¼ì— ì €ì¥
                while (shortArray.hasRemaining()) {
                    pendingBuffer.add(shortArray.get())
                }
                Log.d("deepVoice", "ğŸ”„ ë¶„ì„ ì¤‘... pendingBufferì— ì¶”ê°€ (RMS: %.2f, pending í¬ê¸°: ${pendingBuffer.size})".format(rms))
            } else {
                // âœ… ë¶„ì„ ì•ˆ í•  ë•ŒëŠ” ë©”ì¸ ë²„í¼ì— ì €ì¥
                while (shortArray.hasRemaining()) {
                    audioBuffer.add(shortArray.get())
                }
                Log.d("deepVoice", "ğŸ¤ audioBufferì— ì¶”ê°€ (RMS: %.2f, buffer í¬ê¸°: ${audioBuffer.size}/${TARGET_BUFFER_SIZE})".format(rms))
            }
        }

        // ë²„í¼ê°€ 3ì´ˆì¹˜ ìŒ“ì˜€ëŠ”ì§€ í™•ì¸
        analyzeIfReady()
    }

    /**
     * RMS ì—ë„ˆì§€ ë ˆë²¨ì„ ê¸°ë°˜ìœ¼ë¡œ ìŒì„± í™œë™ì„ ê°ì§€í•©ë‹ˆë‹¤.
     * @param rms RMS ì—ë„ˆì§€ ê°’
     * @return true: ìŒì„± ìˆìŒ, false: ë¬´ìŒ
     */
    private fun isVoiceActive(rms: Double): Boolean {
        return rms > RMS_THRESHOLD
    }

    /**
     * RMS (Root Mean Square) ì—ë„ˆì§€ ê³„ì‚°
     * @param shortBuffer Short ë°°ì—´
     * @return RMS ê°’
     */
    private fun calculateRMS(shortBuffer: ShortBuffer): Double {
        shortBuffer.rewind() // ì²˜ìŒë¶€í„° ì½ê¸° ì‹œì‘

        var sum = 0.0
        var count = 0

        while (shortBuffer.hasRemaining()) {
            val sample = shortBuffer.get()
            sum += (sample * sample).toDouble()
            count++
        }

        return if (count > 0) {
            kotlin.math.sqrt(sum / count)
        } else {
            0.0
        }
    }

    /**
     * ë²„í¼ê°€ 3ì´ˆì¹˜(48000 ìƒ˜í”Œ) ìŒ“ì˜€ìœ¼ë©´ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.
     * íŠ¬ë¸”ë§ ìœˆë„ìš° ë°©ì‹: ë¶„ì„ ì‹œì‘ ì‹œ ë²„í¼ë¥¼ ë¹„ì›€
     */
    private fun analyzeIfReady() {
        synchronized(audioBuffer) {
            // ë²„í¼ê°€ ì¶©ë¶„íˆ ìŒ“ì´ì§€ ì•Šì•˜ìœ¼ë©´ ë¦¬í„´
            if (audioBuffer.size < TARGET_BUFFER_SIZE) {
                return
            }

            // ì´ë¯¸ ë¶„ì„ ì¤‘ì´ë©´ SKIP
            if (isAnalyzing.get()) {
                return
            }

            // ë¶„ì„ ì‹œì‘
            isAnalyzing.set(true)

            // ë²„í¼ì˜ ìŠ¤ëƒ…ìƒ·ì„ ë§Œë“¦ (ì •í™•íˆ 3ì´ˆ ë¶„ëŸ‰)
            val audioData = audioBuffer.toShortArray()

            // âœ… íŠ¬ë¸”ë§ ìœˆë„ìš°: ë¶„ì„ ì‹œì‘ ì‹œ ë²„í¼ë¥¼ ë¹„ì›€!
            audioBuffer.clear()

            Log.d("deepVoice", "ğŸ¤ íŠ¬ë¸”ë§ ìœˆë„ìš° ë¶„ì„ ì‹œì‘ (ìƒ˜í”Œ ìˆ˜: ${audioData.size}, ë²„í¼ ë¹„ì›€)")

            // ë¹„ë™ê¸°ë¡œ ì¶”ë¡  ì‹¤í–‰
            runInference(audioData)
        }
    }

    /**
     * 3ì´ˆ ë²„í¼ê°€ ëŒ€ë¶€ë¶„ ë¬´ìŒì¸ì§€ ì •êµí•˜ê²Œ íŒë‹¨í•©ë‹ˆë‹¤.
     * @param audioData 3ì´ˆ ë¶„ëŸ‰ì˜ ì˜¤ë””ì˜¤ ë°ì´í„° (ShortArray)
     * @return ë‘ ê°€ì§€ ì¡°ê±´ì„ ëª¨ë‘ ë§Œì¡±í•˜ë©´ true (ëŒ€ë¶€ë¶„ ë¬´ìŒ), ì•„ë‹ˆë©´ false
     */
    private fun isBufferMostlySilent(audioData: ShortArray): Boolean {
        // --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ---
        // ê°œë³„ ì²­í¬ì˜ RMSê°€ ì´ ê°’ë³´ë‹¤ ë‚®ìœ¼ë©´ 'ë¬´ìŒ ì²­í¬'ë¡œ ê°„ì£¼
        val INDIVIDUAL_RMS_THRESHOLD = 5.0
        // 3ì´ˆ ë²„í¼ ë‚´ 'ë¬´ìŒ ì²­í¬'ì˜ ë¹„ìœ¨ì´ ì´ ê°’ì„ ì´ˆê³¼í•´ì•¼ í•¨
        val SILENT_CHUNK_RATIO_THRESHOLD = 2.0 / 3.0
        // 3ì´ˆ ë²„í¼ ì „ì²´ì˜ í‰ê·  RMSê°€ ì´ ê°’ë³´ë‹¤ ë‚®ì•„ì•¼ í•¨
        val AVERAGE_RMS_THRESHOLD = 15.0
        // RMSë¥¼ ê³„ì‚°í•  ê°œë³„ ì²­í¬ì˜ í¬ê¸° (20ms = 320 ìƒ˜í”Œ)
        val CHUNK_SIZE_SAMPLES = 320
        // ---

        if (audioData.isEmpty()) return true

        val numChunks = audioData.size / CHUNK_SIZE_SAMPLES
        if (numChunks == 0) return true

        var silentChunkCount = 0
        var totalRmsSum = 0.0

        for (i in 0 until numChunks) {
            val chunkStart = i * CHUNK_SIZE_SAMPLES
            val chunkEnd = chunkStart + CHUNK_SIZE_SAMPLES
            val chunk = audioData.sliceArray(chunkStart until chunkEnd)
            val chunkBuffer = ShortBuffer.wrap(chunk)

            val chunkRms = calculateRMS(chunkBuffer)
            totalRmsSum += chunkRms

            if (chunkRms < INDIVIDUAL_RMS_THRESHOLD) {
                silentChunkCount++
            }
        }

        val averageRms = totalRmsSum / numChunks
        val silentRatio = silentChunkCount.toDouble() / numChunks

        Log.d(TAG, "VAD Stats: Avg RMS=%.2f, Silent Ratio=%.2f (%d/%d)".format(averageRms, silentRatio, silentChunkCount, numChunks))

        // ë‘ ê°€ì§€ ì¡°ê±´ ëª¨ë‘ ë§Œì¡± ì‹œ 'ëŒ€ë¶€ë¶„ ë¬´ìŒ'ìœ¼ë¡œ íŒë‹¨
        val isMostlySilent = averageRms < AVERAGE_RMS_THRESHOLD && silentRatio > SILENT_CHUNK_RATIO_THRESHOLD
        if (isMostlySilent) {
            Log.d(TAG, "ğŸ”‡ 3ì´ˆ ë²„í¼ê°€ ëŒ€ë¶€ë¶„ ë¬´ìŒìœ¼ë¡œ íŒë‹¨ë˜ì–´ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        }

        return isMostlySilent
    }

    /**
     * PTL ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸°ë¡œ ì¶”ë¡ ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
     */
    private fun runInference(audioData: ShortArray) {
        CoroutineScope(Dispatchers.IO).launch {
            try {
                // âœ… [ìˆ˜ì •] ì •êµí•œ ë¬´ìŒ êµ¬ê°„ ìŠ¤í‚µ ë¡œì§
                if (isBufferMostlySilent(audioData)) {
                    // onResult ì½œë°±ì„ í˜¸ì¶œí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, UIëŠ” ë³€ê²½ë˜ì§€ ì•Šê³  ì´ì „ ìƒíƒœë¥¼ ìœ ì§€í•¨
                    return@launch
                }
                // âœ… ë¬´ìŒ êµ¬ê°„ ìŠ¤í‚µ ë¡œì§ ë

                if (model == null) {
                    Log.e(TAG, "âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    isAnalyzing.set(false)
                    return@launch
                }

                // 1. Short -> Float ë³€í™˜ ë° ì •ê·œí™”
                val floatArray = FloatArray(audioData.size)
                for (i in audioData.indices) {
                    floatArray[i] = audioData[i].toFloat() / Short.MAX_VALUE // -1.0 ~ 1.0 ì •ê·œí™”
                }

                // 2. Zero-mean, Unit-variance ì •ê·œí™”
                // âœ… PyTorch tensor.std()ì™€ ë™ì¼í•˜ê²Œ unbiased=True (n-1ë¡œ ë‚˜ëˆ”) ì ìš©
                val mean = floatArray.average().toFloat()

                // âœ… Bessel's correction ì ìš©: n-1ë¡œ ë‚˜ëˆ” (unbiased variance)
                var sumSquaredDiff = 0.0
                for (i in floatArray.indices) {
                    val diff = floatArray[i] - mean
                    sumSquaredDiff += (diff * diff).toDouble()
                }

                val n = floatArray.size
                val variance = if (n > 1) {
                    (sumSquaredDiff / (n - 1)).toFloat()  // â† n-1ë¡œ ë‚˜ëˆ” (unbiased)
                } else {
                    0f
                }
                val std = kotlin.math.sqrt(variance)

                // ì •ê·œí™” ì ìš© (Python Colabê³¼ ë™ì¼)
                for (i in floatArray.indices) {
                    floatArray[i] = (floatArray[i] - mean) / (std + 1e-5f)
                }

                Log.d(TAG, "ğŸ“Š ì •ê·œí™” í†µê³„ (unbiased): mean=$mean, std=$std, n=$n")

                // 3. Tensor ìƒì„± [1, 48000] (3ì´ˆ ë¶„ëŸ‰)
                val inputTensor = Tensor.fromBlob(
                    floatArray,
                    longArrayOf(1, audioData.size.toLong())
                )

                // 4. ëª¨ë¸ ì¶”ë¡ 
                val startTime = System.currentTimeMillis()
                val startCpuTime = SystemClock.currentThreadTimeMillis()

                val outputTensor = model!!.forward(IValue.from(inputTensor)).toTensor()

                val endCpuTime = SystemClock.currentThreadTimeMillis()
                val endTime = System.currentTimeMillis()

                val inferenceTime = endTime - startTime
                val cpuTime = (endCpuTime - startCpuTime) / 1_000_000 // ns to ms

                // ì‹¤ì‹œê°„ ì„±ëŠ¥ ë¡œê·¸
                Log.i(TAG, "Real-time - Inference Time: ${inferenceTime}ms, CPU Time: ${cpuTime}ms")

                // í†µê³„ ì—…ë°ì´íŠ¸
                synchronized(this) {
                    inferenceCount++
                    totalInferenceTimeMs += inferenceTime
                    totalCpuTimeMs += cpuTime
                }

                val scores = outputTensor.dataAsFloatArray // [1, 2] -> [real_score, fake_score]

                // 5. Softmax ì ìš©
                val expSum = kotlin.math.exp(scores[0].toDouble()) + kotlin.math.exp(scores[1].toDouble())
                val probReal = (kotlin.math.exp(scores[0].toDouble()) / expSum).toFloat()
                val probFake = (kotlin.math.exp(scores[1].toDouble()) / expSum).toFloat()

                // 6. ê²°ê³¼ íŒì • (í™•ë¥ ì´ ë†’ì€ ìª½ ì„ íƒ)
                val isReal = probReal > probFake
                val confidence = if (isReal) probReal else probFake

                Log.d("deepFakeCheck", "ğŸ“Š ë¶„ì„ ê²°ê³¼: ${if (isReal) "âœ… ì§„ì§œ" else "âš ï¸ ë”¥í˜ì´í¬"} (ì‹ ë¢°ë„: ${(confidence * 100).toInt()}%)")
                Log.d("deepVoice", "   Real: ${(probReal * 100).toInt()}%, Fake: ${(probFake * 100).toInt()}%")

                // 7. ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ì½œë°± í˜¸ì¶œ
                withContext(Dispatchers.Main) {
                    onResult(isReal, confidence)
                }

            } catch (e: Exception) {
                Log.e(TAG, "âŒ ì¶”ë¡  ì‹¤íŒ¨", e)
            } finally {
                synchronized(audioBuffer) {
                    // âœ… ë¶„ì„ ì™„ë£Œ í›„ pendingBuffer â†’ audioBuffer ì´ë™
                    if (pendingBuffer.isNotEmpty()) {
                        audioBuffer.addAll(pendingBuffer)
                        Log.d("deepVoice", "âœ… ë¶„ì„ ì™„ë£Œ! pendingBuffer(${pendingBuffer.size} ìƒ˜í”Œ)ë¥¼ audioBufferë¡œ ì´ë™")
                        pendingBuffer.clear()
                    } else {
                        Log.d("deepVoice", "âœ… ë¶„ì„ ì™„ë£Œ! (pendingBuffer ë¹„ì–´ìˆìŒ)")
                    }

                    // ë¶„ì„ ì™„ë£Œ, í”Œë˜ê·¸ í•´ì œ
                    isAnalyzing.set(false)
                }
            }
        }
    }

    /**
     * ë¦¬ì†ŒìŠ¤ ì •ë¦¬
     */
    fun cleanup() {
        try {
            model?.destroy()
            audioBuffer.clear()
            pendingBuffer.clear()
            Log.d(TAG, "DeepfakeDetector ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        } catch (e: Exception) {
            Log.e(TAG, "ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜", e)
        }
    }

    /**
     * í†µê³„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ê³  ì¹´ìš´í„°ë¥¼ ë¦¬ì…‹í•©ë‹ˆë‹¤.
     */
    fun getAndResetStats(): String {
        synchronized(this) {
            val avgInferenceTime = if (inferenceCount > 0) totalInferenceTimeMs / inferenceCount else 0
            val report = "DeepfakeDetector Report: Ran $inferenceCount times | " +
                         "Avg Inference Time: ${avgInferenceTime}ms | " +
                         "Total CPU Time: ${totalCpuTimeMs}ms"

            // í†µê³„ ë¦¬ì…‹
            inferenceCount = 0L
            totalInferenceTimeMs = 0L
            totalCpuTimeMs = 0L

            return report
        }
    }
}
